{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('~/datasets/nlp/ori.xlsx')\n",
    "df.fillna('NULL', inplace=True)\n",
    "df.columns = ['id', 'content', 'subject', 'object', 'issue', 'platform', 'amount']\n",
    "df['extract'] = df.apply(lambda x: f\"抽取主体={x['subject']}, 抽取客体={x['object']}, 抽取问题描述={x['issue']}, 抽取平台={x['platform']}, 抽取金额={x['amount']}\", axis=1)\n",
    "\n",
    "\n",
    "lengths = df['extract'].str.len().tolist()\n",
    "sorted(lengths, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "(6500, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deep_translator import MyMemoryTranslator\n",
    "ta = MyMemoryTranslator(source='zh-CN', target='en-US')\n",
    "tb = MyMemoryTranslator(source='en-US', target='zh-CN')\n",
    "import json\n",
    "\n",
    "df = pd.read_excel('~/datasets/nlp/rev.xlsx')\n",
    "df.fillna('NULL', inplace=True)\n",
    "df.columns = ['id', 'content', 'subject', 'object', 'issue', 'platform', 'amount']\n",
    "df['extract'] = df.apply(lambda x: f\"抽取主体={x['subject']}, 抽取客体={x['object']}, 抽取问题描述={x['issue']}, 抽取平台={x['platform']}, 抽取金额={x['amount']}\", axis=1)\n",
    "prompt='你的任务是为接下来这段内容抽取[主体、客体、问题描述、平台、金额]，以#抽取主体=..., 抽取客体=..., 抽取问题描述=..., 抽取平台=..., 抽取金额=...#的格式给出，没有找到的内容请填上null'\n",
    "case='例子：输入#刘爽投诉：2022年12月11日在淘宝平台仙桃市龙华山菜帮村湖北法保瑞防护科技有限公司n95口罩共计57.18元，商家承诺是48小时内发货，现商家迟迟不发货，请市监局调处！#，回答#抽取主体=湖北法保瑞防护科技有限公司, 抽取客体=口罩, 抽取问题描述=商家迟迟不发货, 抽取平台=淘宝, 抽取金额=57.18元#'\n",
    "\n",
    "def random_insertion_deletion_replacement(text, n=5):\n",
    "    words = text.split()\n",
    "    for _ in range(n):\n",
    "        if words:\n",
    "            new_word = random.choice(words)\n",
    "            index = random.randint(0, len(words))\n",
    "            words.insert(index, new_word)\n",
    "        if len(words) > 1:\n",
    "            del words[random.randint(0, len(words) - 1)]\n",
    "        if len(words) > 1:\n",
    "            index1, index2 = random.randint(0, len(words) - 1), random.randint(0, len(words) - 1)\n",
    "            words[index1], words[index2] = words[index2], words[index1]\n",
    "    return ' '.join(words)\n",
    "\n",
    "augmented_data = []\n",
    "for i, row in df.iloc[500:1500].iterrows():\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    new_content = random_insertion_deletion_replacement(row['content'])\n",
    "    augmented_data.append({'content': new_content, 'extract': row['extract']})\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df['question'] = prompt+'\\n'+case+'\\n输入内容如下#\\n'+ df['content']\n",
    "\n",
    "df = df[['question', 'extract']]  # 保留两列\n",
    "\n",
    "def save_json_lines(df, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for record in df.to_dict(orient='records'):\n",
    "            json_str = json.dumps(record, ensure_ascii=False)\n",
    "            file.write(json_str + '\\n')\n",
    "\n",
    "save_json_lines(df, 'all.json')\n",
    "a=df.iloc[:1500]\n",
    "b = df.iloc[1500:5000]\n",
    "c = df.iloc[5000:7000]\n",
    "train_df, test_df = train_test_split(b, test_size=0.2, random_state=42)\n",
    "train_df = pd.concat([a, train_df, c])\n",
    "\n",
    "save_json_lines(train_df, 'train.json')\n",
    "save_json_lines(test_df, 'test.json')\n",
    "\n",
    "small_df = test_df.head(int(48))\n",
    "save_json_lines(small_df, 'small.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: /data/yanghq/nlp_ptuning/pre/all.json\n",
      "Deleted: /data/yanghq/nlp_ptuning/pre/small.json\n",
      "Deleted: /data/yanghq/nlp_ptuning/pre/train.json\n",
      "Deleted: /data/yanghq/nlp_ptuning/pre/test.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def delete_json_files(directory):\n",
    "    # 遍历指定目录\n",
    "    for filename in os.listdir(directory):\n",
    "        # 检查文件扩展名是否为.json\n",
    "        if filename.endswith('.json'):\n",
    "            # 构建完整的文件路径\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            # 删除文件\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "\n",
    "# 指定你的目录路径\n",
    "path_to_directory = '/data/yanghq/nlp_ptuning/pre/'\n",
    "\n",
    "# 调用函数\n",
    "delete_json_files(path_to_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
